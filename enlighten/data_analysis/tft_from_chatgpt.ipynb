{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TemporalFusionTransformer(tf.keras.Model):\n",
    "    def __init__(self, num_inputs, num_outputs, num_features, num_layers, d_model, num_heads, dff, dropout_rate):\n",
    "        super(TemporalFusionTransformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_inputs, num_features, num_layers, d_model, num_heads, dff, dropout_rate)\n",
    "        self.decoder = Decoder(num_features, num_layers, d_model, num_heads, dff, dropout_rate)\n",
    "        self.final_layer = layers.Dense(num_outputs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, static_inputs = inputs\n",
    "\n",
    "        # Encode the temporal inputs\n",
    "        encoded = self.encoder(x)\n",
    "\n",
    "        # Concatenate the encoded temporal inputs with the static inputs\n",
    "        fused = tf.concat([encoded, static_inputs], axis=-1)\n",
    "\n",
    "        # Decode the fused inputs\n",
    "        decoded = self.decoder(fused)\n",
    "\n",
    "        # Pass the decoded outputs through the final layer\n",
    "        outputs = self.final_layer(decoded)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Define the Encoder layer\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_inputs, num_features, num_layers, d_model, num_heads, dff, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_inputs = num_inputs\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = layers.Dense(d_model)\n",
    "        self.positional_encoding = positional_encoding(num_inputs, d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.positional_encoding[:, :self.num_inputs, :]\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define the Encoder layer\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = layers.MultiHeadAttention(num_heads, d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attention_output = self.mha(inputs, inputs)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        x1 = self.layer_norm1(inputs + attention_output)\n",
    "\n",
    "        ffn_output = self.ffn(x1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        x2 = self.layer_norm2(x1 + ffn_output)\n",
    "\n",
    "        return x2\n",
    "\n",
    "# Define the Decoder layer\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_features, num_layers, d_model, num_heads, dff, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_features = num_features\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate) for _ in range(num_layers)]\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x)\n",
    "\n",
    "        x = self.dropout(x[:, -self.num_features:, :])\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define the Decoder layer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = layers.MultiHeadAttention(num_heads, d_model)\n",
    "        self.mha2 = layers.MultiHeadAttention(num_heads, d_model)\n",
    "\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(dff, activation='relu'),\n",
    "            layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, encoder_outputs = inputs\n",
    "\n",
    "        attn1 = self.mha1(x, x)\n",
    "        attn1 = self.dropout1(attn1)\n",
    "        x1 = self.layer_norm1(attn1 + x)\n",
    "\n",
    "        attn2 = self.mha2(x1, encoder_outputs)\n",
    "        attn2 = self.dropout2(attn2)\n",
    "        x2 = self.layer_norm2(attn2 + x1)\n",
    "\n",
    "        ffn_output = self.ffn(x2)\n",
    "        ffn_output = self.dropout3(ffn_output)\n",
    "        x3 = self.layer_norm3(ffn_output + x2)\n",
    "\n",
    "        return x3\n",
    "\n",
    "# Define the positional encoding\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "\n",
    "    # Apply sin to even indices in the array\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # Apply cos to odd indices in the array\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "# Utility function to calculate angles for the positional encoding\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "# Instantiate the Temporal Fusion Transformer model\n",
    "num_inputs = 10\n",
    "num_outputs = 1\n",
    "num_features = 5\n",
    "num_layers = 2\n",
    "d_model = 64\n",
    "num_heads = 2\n",
    "dff = 128\n",
    "dropout_rate = 0.1\n",
    "\n",
    "tft_model = TemporalFusionTransformer(num_inputs, num_outputs, num_features, num_layers, d_model, num_heads, dff, dropout_rate)\n",
    "\n",
    "# Compile the model\n",
    "tft_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "tft_model.fit([temporal_inputs, static_inputs], targets, epochs=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
